{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31da1e59-3996-4649-bb4c-a0f1913bbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv('output_data.csv')\n",
    "\n",
    "# Create the first dataset with the first two columns\n",
    "original_dataset = data.iloc[:, :2]\n",
    "\n",
    "# Create the second dataset with the third and fourth columns\n",
    "synthesized_datasets = data.iloc[:, 2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5505012a-9726-43fe-bb3a-ffa48b77df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import random\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21ac6158-8225-49b3-8985-91669b3c8118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4116727452c47c5bb6ba247395aba26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Fix the padding token issue\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set eos_token as the pad_token\n",
    "\n",
    "# Load the model without quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # Automatically maps the model to available GPUs\n",
    "    torch_dtype=torch.float16  # Use mixed precision for better performance\n",
    ")\n",
    "\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30280a78-681d-47a1-ac61-36683fd0c854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions:\n",
      "  M_ori: 1.0000\n",
      "  M_ref: 0.3133\n",
      "  Delta (∆): 0.6867\n",
      "  Delta Relative (δ): 68.6661\n",
      "\n",
      "Answers:\n",
      "  M_ori: 1.0000\n",
      "  M_ref: 0.2834\n",
      "  Delta (∆): 0.7166\n",
      "  Delta Relative (δ): 71.6597\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_ngram_accuracy(reference, generated, n=2):\n",
    "    \"\"\"\n",
    "    Calculate N-gram accuracy between a reference and generated text.\n",
    "    \"\"\"\n",
    "    ref_ngrams = set([' '.join(reference[i:i + n]) for i in range(len(reference) - n + 1)])\n",
    "    gen_ngrams = set([' '.join(generated[i:i + n]) for i in range(len(generated) - n + 1)])\n",
    "    matches = len(ref_ngrams & gen_ngrams)\n",
    "    return matches / len(ref_ngrams) if len(ref_ngrams) > 0 else 0.0\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize text into words.\n",
    "    \"\"\"\n",
    "    return text.split()\n",
    "\n",
    "def evaluate_atomic_metric(original_dataset, synthesized_dataset, n=2):\n",
    "    \"\"\"\n",
    "    Evaluate the atomic metric (N-gram accuracy) and calculate decrement (∆) and percentage decrease (δ).\n",
    "    \"\"\"\n",
    "    if len(original_dataset) != len(synthesized_dataset):\n",
    "        raise ValueError(\"Original and synthesized datasets must have the same number of rows.\")\n",
    "\n",
    "    # N-gram accuracies for the original dataset\n",
    "    original_accuracies_question = []\n",
    "    original_accuracies_answer = []\n",
    "    \n",
    "    for _, row in original_dataset.iterrows():\n",
    "        original_question = process_text(row[\"Original Question\"])\n",
    "        original_answer = process_text(row[\"Original Answer\"])\n",
    "        \n",
    "        # Use original dataset against itself for full similarity (optional)\n",
    "        question_accuracy = calculate_ngram_accuracy(original_question, original_question, n=n)\n",
    "        answer_accuracy = calculate_ngram_accuracy(original_answer, original_answer, n=n)\n",
    "        \n",
    "        original_accuracies_question.append(question_accuracy)\n",
    "        original_accuracies_answer.append(answer_accuracy)\n",
    "    \n",
    "    M_ori_question = np.mean(original_accuracies_question)\n",
    "    M_ori_answer = np.mean(original_accuracies_answer)\n",
    "\n",
    "    # N-gram accuracies for the synthesized dataset\n",
    "    synthesized_accuracies_question = []\n",
    "    synthesized_accuracies_answer = []\n",
    "    \n",
    "    for (_, orig_row), (_, synth_row) in zip(original_dataset.iterrows(), synthesized_dataset.iterrows()):\n",
    "        original_question = process_text(orig_row[\"Original Question\"])\n",
    "        synthesized_question = process_text(synth_row[\"Predicted Question\"])\n",
    "        original_answer = process_text(orig_row[\"Original Answer\"])\n",
    "        synthesized_answer = process_text(synth_row[\"Rewritten Answer\"])\n",
    "        \n",
    "        question_accuracy = calculate_ngram_accuracy(original_question, synthesized_question, n=n)\n",
    "        answer_accuracy = calculate_ngram_accuracy(original_answer, synthesized_answer, n=n)\n",
    "        \n",
    "        synthesized_accuracies_question.append(question_accuracy)\n",
    "        synthesized_accuracies_answer.append(answer_accuracy)\n",
    "    \n",
    "    M_ref_question = np.mean(synthesized_accuracies_question)\n",
    "    M_ref_answer = np.mean(synthesized_accuracies_answer)\n",
    "\n",
    "    # Calculate decrement (∆) and percentage decrease (δ)\n",
    "    delta_question = M_ori_question - M_ref_question\n",
    "    delta_answer = M_ori_answer - M_ref_answer\n",
    "\n",
    "    delta_relative_question = (delta_question / M_ori_question) * 100 if M_ori_question != 0 else 0\n",
    "    delta_relative_answer = (delta_answer / M_ori_answer) * 100 if M_ori_answer != 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Questions\": {\n",
    "            \"M_ori\": M_ori_question,\n",
    "            \"M_ref\": M_ref_question,\n",
    "            \"Delta (∆)\": delta_question,\n",
    "            \"Delta Relative (δ)\": delta_relative_question\n",
    "        },\n",
    "        \"Answers\": {\n",
    "            \"M_ori\": M_ori_answer,\n",
    "            \"M_ref\": M_ref_answer,\n",
    "            \"Delta (∆)\": delta_answer,\n",
    "            \"Delta Relative (δ)\": delta_relative_answer\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Example Usage\n",
    "# Calculate metrics\n",
    "results = evaluate_atomic_metric(original_dataset, synthesized_datasets, n=2)\n",
    "\n",
    "# Display results\n",
    "print(\"Questions:\")\n",
    "for key, value in results[\"Questions\"].items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nAnswers:\")\n",
    "for key, value in results[\"Answers\"].items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "929a9dd8-d1bb-42c3-b121-83f910602e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.31333891190181756, 0.28340289504629607)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_atomic_metric_on_benchmarks(model, tokenizer, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c52ddb-2be0-48e2-b908-520c2ddc352e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
