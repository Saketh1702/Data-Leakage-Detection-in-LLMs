# Data Leakage Detection in LLMs

A framework for detecting data leakage and bias in LLMs (e.g., Llama-2, Mistral) using n-gram metrics and one-shot prompting. BLEURT and ROUGE-L models are used to evaluate similarity between reference and model outputs for guided and general prompts. The framework analyzes model behavior on MMLU and TruthfulQA benchmarks to identify training data memorization and gender stereotyping patterns.

